{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Stage Cat Identification Pipeline ðŸš€\n",
    "\n",
    "This notebook implements a two-stage computer vision pipeline:\n",
    "1.  **Cat Detector:** A YOLO model trained to find any cat in an image.\n",
    "2.  **Cat Classifier:** A second model trained to identify if a cropped image of a cat is **Marnie** or **Milo**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Master Configuration âš™ï¸\n",
    "**Action:** Set all the main variables in this cell before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:25:18.085168Z",
     "start_time": "2025-10-16T02:25:16.850762Z"
    }
   },
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import zipfile\n",
    "import yaml\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 1. Project & File Configuration ---\n",
    "PROJECT_NAME = \"Cat_Pipeline_v1\"\n",
    "EXPORTED_ZIP_PATH = 'C:\\\\Users\\\\josia\\\\Downloads\\\\project-2-at-2025-10-07-23-35-a592af58.zip' # Path to your Label Studio YOLO export\n",
    "\n",
    "# --- 2. Dataset Path Configuration ---\n",
    "# These folders will be created and organized by the notebook\n",
    "BASE_DATA_DIR = 'datasets' # Root folder for all datasets\n",
    "RAW_DATA_PATH = os.path.join(BASE_DATA_DIR, '0_raw_from_zip') # For the initial unzipped data\n",
    "DETECTOR_DATA_PATH = os.path.join(BASE_DATA_DIR, '1_detector_dataset') # For the single-class detector\n",
    "CLASSIFIER_DATA_PATH = os.path.join(BASE_DATA_DIR, '2_classifier_dataset') # For the two-class classifier\n",
    "\n",
    "# --- 3. Dataset & Training Settings ---\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.2\n",
    "DETECTOR_EPOCHS = 150\n",
    "DETECTOR_PATIENCE = 50\n",
    "CLASSIFIER_EPOCHS = 35\n",
    "CLASSIFIER_PATIENCE = 15\n",
    "IMAGE_SIZE = 640\n",
    "\n",
    "# This mapping is from your Label Studio export. It's critical for creating the classifier dataset.\n",
    "CLASS_MAPPING = {0: 'Marnie', 1: 'Milo'} \n",
    "\n",
    "print(\"âœ… Configuration loaded.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Initial Data Prep (From Label Studio) ðŸ“¦\n",
    "--- \n",
    "This section takes the raw export from Label Studio and prepares it for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T01:30:01.781046Z",
     "start_time": "2025-10-16T01:30:01.510029Z"
    }
   },
   "source": [
    "print(f\"Unzipping '{EXPORTED_ZIP_PATH}' to '{RAW_DATA_PATH}'...\")\n",
    "\n",
    "# Clean up previous runs\n",
    "if os.path.exists(RAW_DATA_PATH):\n",
    "    shutil.rmtree(RAW_DATA_PATH)\n",
    "\n",
    "# Unzip the new data\n",
    "with zipfile.ZipFile(EXPORTED_ZIP_PATH, 'r') as zip_ref:\n",
    "    zip_ref.extractall(RAW_DATA_PATH)\n",
    "\n",
    "print(\"âœ… Unzip complete.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping 'C:\\Users\\josia\\Downloads\\project-2-at-2025-10-07-23-35-a592af58.zip' to 'datasets\\0_raw_from_zip'...\n",
      "âœ… Unzip complete.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T01:40:06.697037Z",
     "start_time": "2025-10-16T01:40:06.651661Z"
    }
   },
   "source": [
    "# NOTE: This useful filename cleaning function is from your original notebook.\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def clean_filenames_in_directory(directory_path):\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"  - âš ï¸  Warning: Directory not found, skipping: {directory_path}\")\n",
    "        return 0\n",
    "    renamed_count = 0\n",
    "    for filename in os.listdir(directory_path):\n",
    "        new_filename = None\n",
    "        if '__' in filename:\n",
    "            _, main_part = filename.split('__', 1)\n",
    "            new_filename = unquote(main_part)\n",
    "        elif '-' in filename:\n",
    "            parts = filename.split('-', 1)\n",
    "            if len(parts) == 2 and len(parts[0]) == 8 and all(c in '0123456789abcdef' for c in parts[0]):\n",
    "                new_filename = parts[1]\n",
    "        if new_filename and new_filename != filename:\n",
    "            shutil.move(os.path.join(directory_path, filename), os.path.join(directory_path, new_filename))\n",
    "            renamed_count += 1\n",
    "    return renamed_count\n",
    "\n",
    "print(\"--- Starting Filename Cleanup ---\")\n",
    "images_dir_to_clean = os.path.join(RAW_DATA_PATH, 'images')\n",
    "labels_dir_to_clean = os.path.join(RAW_DATA_PATH, 'labels')\n",
    "\n",
    "renamed_images = clean_filenames_in_directory(images_dir_to_clean)\n",
    "print(f\"Renamed {renamed_images} image files.\")\n",
    "renamed_labels = clean_filenames_in_directory(labels_dir_to_clean)\n",
    "print(f\"Renamed {renamed_labels} label files.\")\n",
    "print(\"âœ… Filename cleanup complete.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Filename Cleanup ---\n",
      "Renamed 0 image files.\n",
      "Renamed 137 label files.\n",
      "âœ… Filename cleanup complete.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T01:40:09.682290Z",
     "start_time": "2025-10-16T01:40:09.016573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# ==============================================================================\n",
    "# --- âš™ï¸ Configuration ---\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Define the source folder where your combined images are currently located.\n",
    "#    This path should be updated if you change the output of your previous script.\n",
    "SOURCE_IMAGES_FOLDER = 'original images\\\\ALLPhotos_and_frames'\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Script Logic ---\n",
    "\n",
    "# Create the full path to the destination 'images' subfolder\n",
    "destination_images_folder = os.path.join(RAW_DATA_PATH, 'images')\n",
    "\n",
    "print(f\"Preparing to move files...\")\n",
    "print(f\"  - From: {SOURCE_IMAGES_FOLDER}\")\n",
    "print(f\"  - To:   {destination_images_folder}\")\n",
    "\n",
    "# Ensure the destination directory and its parent exist\n",
    "os.makedirs(destination_images_folder, exist_ok=True)\n",
    "\n",
    "# Check if the source directory exists before proceeding\n",
    "if not os.path.isdir(SOURCE_IMAGES_FOLDER):\n",
    "    print(f\"\\nâŒ ERROR: Source directory not found at '{SOURCE_IMAGES_FOLDER}'. Please check the path.\")\n",
    "else:\n",
    "    # Get a list of all files to move\n",
    "    files_to_move = [f for f in os.listdir(SOURCE_IMAGES_FOLDER) if os.path.isfile(os.path.join(SOURCE_IMAGES_FOLDER, f))]\n",
    "\n",
    "    moved_count = 0\n",
    "    # Loop through the files and move each one\n",
    "    for filename in files_to_move:\n",
    "        source_path = os.path.join(SOURCE_IMAGES_FOLDER, filename)\n",
    "        destination_path = os.path.join(destination_images_folder, filename)\n",
    "\n",
    "        # Move the file\n",
    "        shutil.copy(source_path, destination_path)\n",
    "        moved_count += 1\n",
    "\n",
    "    print(f\"\\nâœ… Successfully moved {moved_count} image files.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to move files...\n",
      "  - From: original images\\ALLPhotos_and_frames\n",
      "  - To:   datasets\\0_raw_from_zip\\images\n",
      "\n",
      "âœ… Successfully moved 571 image files.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T01:40:14.419728Z",
     "start_time": "2025-10-16T01:40:14.400764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Set the path to your folders after running prediction\n",
    "images_folder = os.path.join(RAW_DATA_PATH, 'images')\n",
    "labels_folder = os.path.join(RAW_DATA_PATH, 'labels')\n",
    "\n",
    "# --- Script to create empty label files ---\n",
    "\n",
    "# Get the base names (without extension) of all images and labels\n",
    "image_basenames = {os.path.splitext(f)[0] for f in os.listdir(images_folder)}\n",
    "label_basenames = {os.path.splitext(f)[0] for f in os.listdir(labels_folder)}\n",
    "\n",
    "# Find all images that are missing a label file\n",
    "missing_labels = image_basenames - label_basenames\n",
    "\n",
    "print(f\"Found {len(missing_labels)} images without labels. Creating empty .txt files for them...\")\n",
    "\n",
    "# Create an empty .txt file for each missing label\n",
    "for basename in missing_labels:\n",
    "    with open(os.path.join(labels_folder, f\"{basename}.txt\"), 'w') as f:\n",
    "        pass # The 'pass' command creates an empty file\n",
    "\n",
    "print(\"âœ… Done.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images without labels. Creating empty .txt files for them...\n",
      "âœ… Done.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparing the DETECTOR Dataset ðŸŽ¯\n",
    "--- \n",
    "Here, we'll convert the multi-class labels (Marnie, Milo) into a single class ('cat') and split the data for training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T01:30:32.442195Z",
     "start_time": "2025-10-16T01:30:30.706805Z"
    }
   },
   "source": [
    "print(\"Modifying labels for single-class 'cat' detection...\")\n",
    "\n",
    "# Create the new detector dataset structure\n",
    "detector_images_path = os.path.join(DETECTOR_DATA_PATH, 'images')\n",
    "detector_labels_path = os.path.join(DETECTOR_DATA_PATH, 'labels')\n",
    "os.makedirs(detector_images_path, exist_ok=True)\n",
    "os.makedirs(detector_labels_path, exist_ok=True)\n",
    "\n",
    "source_labels_path = os.path.join(RAW_DATA_PATH, 'labels')\n",
    "source_images_path = os.path.join(RAW_DATA_PATH, 'images')\n",
    "\n",
    "modified_count = 0\n",
    "# Copy all images to the new location\n",
    "for filename in os.listdir(source_images_path):\n",
    "    shutil.copy(os.path.join(source_images_path, filename), os.path.join(detector_images_path, filename))\n",
    "\n",
    "# Process and copy label files, changing the class ID to 0\n",
    "for filename in os.listdir(source_labels_path):\n",
    "    with open(os.path.join(source_labels_path, filename), 'r') as f_in:\n",
    "        with open(os.path.join(detector_labels_path, filename), 'w') as f_out:\n",
    "            for line in f_in:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) > 1:\n",
    "                    # Replace class ID (parts[0]) with 0\n",
    "                    new_line = f\"0 {' '.join(parts[1:])}\\n\"\n",
    "                    f_out.write(new_line)\n",
    "                    modified_count += 1\n",
    "\n",
    "print(f\"Processed {modified_count} labels, converting all to class '0' for the detector.\")\n",
    "print(\"âœ… Detector dataset created.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modifying labels for single-class 'cat' detection...\n",
      "Processed 532 labels, converting all to class '0' for the detector.\n",
      "âœ… Detector dataset created.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T01:38:00.112326Z",
     "start_time": "2025-10-16T01:37:57.616506Z"
    }
   },
   "source": [
    "print(\"Splitting the detector dataset...\")\n",
    "\n",
    "# Get all image filenames\n",
    "all_files = [f for f in os.listdir(detector_images_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "random.shuffle(all_files)\n",
    "\n",
    "# Calculate split index\n",
    "train_idx = int(len(all_files) * TRAIN_RATIO)\n",
    "train_files = all_files[:train_idx]\n",
    "val_files = all_files[train_idx:]\n",
    "\n",
    "# Function to copy files\n",
    "def copy_split_files(file_list, split_name):\n",
    "    split_images_dir = os.path.join(DETECTOR_DATA_PATH, 'images', split_name)\n",
    "    split_labels_dir = os.path.join(DETECTOR_DATA_PATH, 'labels', split_name)\n",
    "    os.makedirs(split_images_dir, exist_ok=True)\n",
    "    os.makedirs(split_labels_dir, exist_ok=True)\n",
    "    \n",
    "    for filename in file_list:\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        shutil.copy(os.path.join(detector_images_path, filename), os.path.join(split_images_dir, filename))\n",
    "        shutil.copy(os.path.join(detector_labels_path, f\"{base_name}.txt\"), os.path.join(split_labels_dir, f\"{base_name}.txt\"))\n",
    "\n",
    "# Perform the split and move files\n",
    "copy_split_files(train_files, 'train')\n",
    "copy_split_files(val_files, 'val')\n",
    "\n",
    "# Clean up the temporary flat directories\n",
    "for f in os.listdir(detector_images_path):\n",
    "    if os.path.isfile(os.path.join(detector_images_path, f)): os.remove(os.path.join(detector_images_path, f))\n",
    "for f in os.listdir(detector_labels_path):\n",
    "    if os.path.isfile(os.path.join(detector_labels_path, f)): os.remove(os.path.join(detector_labels_path, f))\n",
    "    \n",
    "print(f\"âœ… Dataset split complete. Train: {len(train_files)}, Val: {len(val_files)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the detector dataset...\n",
      "âœ… Dataset split complete. Train: 456, Val: 115\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T01:38:03.834829Z",
     "start_time": "2025-10-16T01:38:03.829406Z"
    }
   },
   "source": [
    "print(\"Creating detector_data.yaml file...\")\n",
    "\n",
    "detector_yaml_path = os.path.join(DETECTOR_DATA_PATH, 'detector_data.yaml')\n",
    "\n",
    "data_for_yaml = {\n",
    "    'train': './images/train',\n",
    "    'val': './images/val',\n",
    "    'nc': 1,\n",
    "    'names': ['cat']\n",
    "}\n",
    "\n",
    "with open(detector_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_for_yaml, f, sort_keys=False, indent=4)\n",
    "\n",
    "print(f\"âœ… YAML file created at: {detector_yaml_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating detector_data.yaml file...\n",
      "âœ… YAML file created at: datasets\\1_detector_dataset\\detector_data.yaml\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training the Cat DETECTOR (Model 1) ðŸš€\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load a pre-trained object detection model\n",
    "detector_model = YOLO('yolo11m.pt')\n",
    "\n",
    "print(\"ðŸš€ Starting Cat Detector training...\")\n",
    "detector_model.train(\n",
    "    data=os.path.join(DETECTOR_DATA_PATH, 'detector_data.yaml'),\n",
    "    epochs=DETECTOR_EPOCHS,\n",
    "    imgsz=IMAGE_SIZE,\n",
    "    project=PROJECT_NAME,\n",
    "    name='detector_model',\n",
    "    exist_ok=True,\n",
    "    batch=.85,\n",
    "    patience=DETECTOR_PATIENCE\n",
    ")\n",
    "\n",
    "print(\"âœ… Detector training complete!\")\n",
    "DETECTOR_WEIGHTS_PATH = os.path.join(PROJECT_NAME, 'detector_model/weights/best.pt')\n",
    "print(f\"Best detector model saved at: {DETECTOR_WEIGHTS_PATH}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Preparing the CLASSIFIER Dataset ðŸ·ï¸\n",
    "--- \n",
    "Now, we'll use the original labels and images to create a new dataset of cropped cat pictures, organized by name for classification training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T01:40:26.892747Z",
     "start_time": "2025-10-16T01:40:21.471243Z"
    }
   },
   "source": [
    "print(\"Creating cropped image dataset for the classifier...\")\n",
    "\n",
    "# Clean up previous runs\n",
    "if os.path.exists(CLASSIFIER_DATA_PATH):\n",
    "    shutil.rmtree(CLASSIFIER_DATA_PATH)\n",
    "\n",
    "def create_cropped_images(split_name, source_files):\n",
    "    # Create subdirectories (e.g., train/Marnie, train/Milo)\n",
    "    for class_name in CLASS_MAPPING.values():\n",
    "        os.makedirs(os.path.join(CLASSIFIER_DATA_PATH, split_name, class_name), exist_ok=True)\n",
    "\n",
    "    crop_count = 0\n",
    "    for image_filename in source_files:\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        image_path = os.path.join(RAW_DATA_PATH, 'images', image_filename)\n",
    "        label_path = os.path.join(RAW_DATA_PATH, 'labels', f\"{base_name}.txt\")\n",
    "\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                parts = line.strip().split()\n",
    "                class_id = int(parts[0])\n",
    "                x_center, y_center, width, height = map(float, parts[1:])\n",
    "\n",
    "                # Convert YOLO format to pixel coordinates\n",
    "                x1 = int((x_center - width / 2) * w)\n",
    "                y1 = int((y_center - height / 2) * h)\n",
    "                x2 = int((x_center + width / 2) * w)\n",
    "                y2 = int((y_center + height / 2) * h)\n",
    "\n",
    "                # Crop the image\n",
    "                cropped_image = image[y1:y2, x1:x2]\n",
    "\n",
    "                # Save to the correct class folder\n",
    "                class_name = CLASS_MAPPING.get(class_id, 'Unknown')\n",
    "                save_path = os.path.join(CLASSIFIER_DATA_PATH, split_name, class_name, f\"{base_name}_{i}.jpg\")\n",
    "                cv2.imwrite(save_path, cropped_image)\n",
    "                crop_count += 1\n",
    "    return crop_count\n",
    "\n",
    "# Create crops for both train and validation sets to maintain data integrity\n",
    "train_crops = create_cropped_images('train', train_files)\n",
    "val_crops = create_cropped_images('val', val_files)\n",
    "\n",
    "print(f\"âœ… Classifier dataset created. Saved {train_crops} training crops and {val_crops} validation crops.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cropped image dataset for the classifier...\n",
      "âœ… Classifier dataset created. Saved 425 training crops and 107 validation crops.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T01:55:12.165880Z",
     "start_time": "2025-10-16T01:54:57.034447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# ðŸš€ REPLACE your \"Part 4.5\" cell with this improved version.\n",
    "#\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Starting 'Unknown' Data Generation (v2 - with Overlap Check) ---\")\n",
    "\n",
    "# --- âš™ï¸ Configuration ---\n",
    "generic_detector = YOLO('yolo11x.pt')\n",
    "EXCLUDED_CLASSES = {'cat', 'dog'}\n",
    "UNKNOWN_CLASS_NAME = 'Unknown'\n",
    "IOU_THRESHOLD = 0.01 # If a generic box overlaps with a known cat by more than 1%, ignore it.\n",
    "# ------------------------\n",
    "\n",
    "def calculate_iou(boxA, boxB):\n",
    "    \"\"\"Calculates Intersection over Union (IoU) between two bounding boxes.\"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "def get_ground_truth_boxes(label_path, img_width, img_height):\n",
    "    \"\"\"Reads a YOLO label file and converts boxes to pixel coordinates (x1, y1, x2, y2).\"\"\"\n",
    "    if not os.path.exists(label_path):\n",
    "        return []\n",
    "\n",
    "    gt_boxes = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            x_center, y_center, width, height = map(float, parts[1:])\n",
    "            x1 = int((x_center - width / 2) * img_width)\n",
    "            y1 = int((y_center - height / 2) * img_height)\n",
    "            x2 = int((x_center + width / 2) * img_width)\n",
    "            y2 = int((y_center + height / 2) * img_height)\n",
    "            gt_boxes.append([x1, y1, x2, y2])\n",
    "    return gt_boxes\n",
    "\n",
    "def generate_unknown_crops(split_name, source_files):\n",
    "    print(f\"Processing '{split_name}' set for unknown objects...\")\n",
    "    unknown_dir = os.path.join(CLASSIFIER_DATA_PATH, split_name, UNKNOWN_CLASS_NAME)\n",
    "    os.makedirs(unknown_dir, exist_ok=True)\n",
    "\n",
    "    unknown_classes_found = set()\n",
    "\n",
    "    unknown_crop_count = 0\n",
    "    for image_filename in source_files:\n",
    "        image_path = os.path.join(RAW_DATA_PATH, 'images', image_filename)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        label_path = os.path.join(RAW_DATA_PATH, 'labels', f\"{base_name}.txt\")\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None: continue\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        # 1. Get the ground truth boxes for Marnie and Milo\n",
    "        gt_cat_boxes = get_ground_truth_boxes(label_path, w, h)\n",
    "\n",
    "        # 2. Run the generic detector\n",
    "        results = generic_detector(image_path, verbose=False)\n",
    "\n",
    "        for res in results:\n",
    "            if res.boxes is None: continue\n",
    "\n",
    "            for box in res.boxes:\n",
    "                class_id = int(box.cls[0])\n",
    "                class_name = generic_detector.names[class_id]\n",
    "\n",
    "                # First check: is the class something we want to ignore?\n",
    "                if class_name in EXCLUDED_CLASSES:\n",
    "                    continue\n",
    "\n",
    "                generic_box = [int(coord) for coord in box.xyxy[0]]\n",
    "\n",
    "                # 3. Second check: does this box overlap with a known cat?\n",
    "                is_overlap = False\n",
    "                for gt_box in gt_cat_boxes:\n",
    "                    if calculate_iou(generic_box, gt_box) > IOU_THRESHOLD:\n",
    "                        is_overlap = True\n",
    "                        break # Found an overlap, no need to check other gt_boxes\n",
    "\n",
    "                if is_overlap:\n",
    "                    continue # Skip this box because it's probably one of your cats\n",
    "\n",
    "                # 4. If it passes all checks, save it as \"Unknown\"\n",
    "                x1, y1, x2, y2 = generic_box\n",
    "                cropped_image = image[y1:y2, x1:x2]\n",
    "                if cropped_image.size > 0:\n",
    "                    save_path = os.path.join(unknown_dir, f\"unknown_{base_name}_{unknown_crop_count}.jpg\")\n",
    "                    cv2.imwrite(save_path, cropped_image)\n",
    "                    unknown_crop_count += 1\n",
    "                    unknown_classes_found.add(class_name)\n",
    "\n",
    "    print(f\"-> Found and saved {unknown_crop_count} 'Unknown' crops for the '{split_name}' set.\")\n",
    "    print(f\"-> Classes saved as 'Unknown': {sorted(list(unknown_classes_found))}\")\n",
    "    return unknown_crop_count\n",
    "\n",
    "# Run the generation process for both your training and validation sets\n",
    "generate_unknown_crops('train', train_files)\n",
    "generate_unknown_crops('val', val_files)\n",
    "\n",
    "print(\"\\nâœ… 'Unknown' class data generation complete.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 'Unknown' Data Generation (v2 - with Overlap Check) ---\n",
      "Processing 'train' set for unknown objects...\n",
      "-> Found and saved 622 'Unknown' crops for the 'train' set.\n",
      "-> Classes saved as 'Unknown': ['bed', 'bottle', 'bowl', 'chair', 'couch', 'laptop', 'person', 'refrigerator', 'remote', 'suitcase', 'tie', 'tv', 'vase']\n",
      "Processing 'val' set for unknown objects...\n",
      "-> Found and saved 159 'Unknown' crops for the 'val' set.\n",
      "-> Classes saved as 'Unknown': ['bed', 'bottle', 'car', 'chair', 'couch', 'potted plant', 'suitcase', 'vase']\n",
      "\n",
      "âœ… 'Unknown' class data generation complete.\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training the Cat CLASSIFIER (Model 2) ðŸš€\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:36:28.186634Z",
     "start_time": "2025-10-16T02:36:23.365229Z"
    }
   },
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load a pre-trained classification model\n",
    "classifier_model = YOLO('yolo11n-cls.pt')\n",
    "\n",
    "print(\"ðŸš€ Starting Cat Classifier training...\")\n",
    "classifier_model.train(\n",
    "    data=CLASSIFIER_DATA_PATH,\n",
    "    epochs=CLASSIFIER_EPOCHS,\n",
    "    imgsz=224, # Classifiers typically use smaller image sizes\n",
    "    project=PROJECT_NAME,\n",
    "    name='classifier_model',\n",
    "    exist_ok=True,\n",
    "    batch=16,\n",
    "    # Lower dropout to a more standard range to allow the model to learn effectively.\n",
    "    dropout=0.3,\n",
    "\n",
    "    # Add weight decay to prevent overfitting.\n",
    "    weight_decay=0.0005,\n",
    "\n",
    "    # Your augmentations are good for this problem.\n",
    "    fliplr=0.5,    # Horizontal flips\n",
    "    flipud=0.1,    # Occasional vertical flips\n",
    "    erasing=0.4,   # Helps model learn from partial images\n",
    ")\n",
    "\n",
    "print(\"âœ… Classifier training complete!\")\n",
    "CLASSIFIER_WEIGHTS_PATH = os.path.join(PROJECT_NAME, 'classifier_model/weights/best.pt')\n",
    "print(f\"Best classifier model saved at: {CLASSIFIER_WEIGHTS_PATH}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Cat Classifier training...\n",
      "New https://pypi.org/project/ultralytics/8.3.214 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.205  Python-3.12.10 torch-2.8.0+cu129 CUDA:0 (NVIDIA GeForce RTX 5070 Ti, 16303MiB)\n",
      "\u001B[34m\u001B[1mengine\\trainer: \u001B[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=datasets\\2_classifier_dataset, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.3, dynamic=False, embed=None, epochs=35, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.1, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=classifier_model, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=Cat_Pipeline_v1, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\josia\\PyCharmMiscProject\\Cat_Pipeline_v1\\classifier_model, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001B[34m\u001B[1mtrain:\u001B[0m C:\\Users\\josia\\PyCharmMiscProject\\datasets\\2_classifier_dataset\\train... found 1047 images in 3 classes  \n",
      "\u001B[34m\u001B[1mval:\u001B[0m C:\\Users\\josia\\PyCharmMiscProject\\datasets\\2_classifier_dataset\\val... found 266 images in 3 classes  \n",
      "\u001B[34m\u001B[1mtest:\u001B[0m None...\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 10                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n",
      "YOLO11n-cls summary: 86 layers, 1,534,947 parameters, 1,534,947 gradients, 3.3 GFLOPs\n",
      "Transferred 234/236 items from pretrained weights\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mchecks passed \n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mFast image access  (ping: 0.00.0 ms, read: 80.639.4 MB/s, size: 4.4 KB)\n",
      "\u001B[K\u001B[34m\u001B[1mtrain: \u001B[0mScanning C:\\Users\\josia\\PyCharmMiscProject\\datasets\\2_classifier_dataset\\train... 1047 images, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1047/1047  0.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      8\u001B[39m classifier_model = YOLO(\u001B[33m'\u001B[39m\u001B[33myolo11n-cls.pt\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     10\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mðŸš€ Starting Cat Classifier training...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m \u001B[43mclassifier_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCLASSIFIER_DATA_PATH\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCLASSIFIER_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mimgsz\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m224\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# Classifiers typically use smaller image sizes\u001B[39;49;00m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mproject\u001B[49m\u001B[43m=\u001B[49m\u001B[43mPROJECT_NAME\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mclassifier_model\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Lower dropout to a more standard range to allow the model to learn effectively.\u001B[39;49;00m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     21\u001B[39m \n\u001B[32m     22\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Add weight decay to prevent overfitting.\u001B[39;49;00m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.0005\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Your augmentations are good for this problem.\u001B[39;49;00m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfliplr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Horizontal flips\u001B[39;49;00m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43mflipud\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Occasional vertical flips\u001B[39;49;00m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43merasing\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m   \u001B[49m\u001B[38;5;66;43;03m# Helps model learn from partial images\u001B[39;49;00m\n\u001B[32m     29\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mâœ… Classifier training complete!\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     32\u001B[39m CLASSIFIER_WEIGHTS_PATH = os.path.join(PROJECT_NAME, \u001B[33m'\u001B[39m\u001B[33mclassifier_model/weights/best.pt\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:800\u001B[39m, in \u001B[36mModel.train\u001B[39m\u001B[34m(self, trainer, **kwargs)\u001B[39m\n\u001B[32m    797\u001B[39m     \u001B[38;5;28mself\u001B[39m.trainer.model = \u001B[38;5;28mself\u001B[39m.trainer.get_model(weights=\u001B[38;5;28mself\u001B[39m.model \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.ckpt \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, cfg=\u001B[38;5;28mself\u001B[39m.model.yaml)\n\u001B[32m    798\u001B[39m     \u001B[38;5;28mself\u001B[39m.model = \u001B[38;5;28mself\u001B[39m.trainer.model\n\u001B[32m--> \u001B[39m\u001B[32m800\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    801\u001B[39m \u001B[38;5;66;03m# Update model and cfg after training\u001B[39;00m\n\u001B[32m    802\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m RANK \u001B[38;5;129;01min\u001B[39;00m {-\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m}:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:235\u001B[39m, in \u001B[36mBaseTrainer.train\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    232\u001B[39m         ddp_cleanup(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mstr\u001B[39m(file))\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m235\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:356\u001B[39m, in \u001B[36mBaseTrainer._do_train\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    354\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.world_size > \u001B[32m1\u001B[39m:\n\u001B[32m    355\u001B[39m     \u001B[38;5;28mself\u001B[39m._setup_ddp()\n\u001B[32m--> \u001B[39m\u001B[32m356\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_setup_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    358\u001B[39m nb = \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m.train_loader)  \u001B[38;5;66;03m# number of batches\u001B[39;00m\n\u001B[32m    359\u001B[39m nw = \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mround\u001B[39m(\u001B[38;5;28mself\u001B[39m.args.warmup_epochs * nb), \u001B[32m100\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.warmup_epochs > \u001B[32m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m -\u001B[32m1\u001B[39m  \u001B[38;5;66;03m# warmup iterations\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:315\u001B[39m, in \u001B[36mBaseTrainer._setup_train\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    313\u001B[39m \u001B[38;5;66;03m# Dataloaders\u001B[39;00m\n\u001B[32m    314\u001B[39m batch_size = \u001B[38;5;28mself\u001B[39m.batch_size // \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mself\u001B[39m.world_size, \u001B[32m1\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m315\u001B[39m \u001B[38;5;28mself\u001B[39m.train_loader = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_dataloader\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m=\u001B[49m\u001B[43mLOCAL_RANK\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m    317\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    318\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m RANK \u001B[38;5;129;01min\u001B[39;00m {-\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m}:\n\u001B[32m    319\u001B[39m     \u001B[38;5;66;03m# Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.\u001B[39;00m\n\u001B[32m    320\u001B[39m     \u001B[38;5;28mself\u001B[39m.test_loader = \u001B[38;5;28mself\u001B[39m.get_dataloader(\n\u001B[32m    321\u001B[39m         \u001B[38;5;28mself\u001B[39m.data.get(\u001B[33m\"\u001B[39m\u001B[33mval\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.data.get(\u001B[33m\"\u001B[39m\u001B[33mtest\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    322\u001B[39m         batch_size=batch_size \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.task == \u001B[33m\"\u001B[39m\u001B[33mobb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m batch_size * \u001B[32m2\u001B[39m,\n\u001B[32m    323\u001B[39m         rank=-\u001B[32m1\u001B[39m,\n\u001B[32m    324\u001B[39m         mode=\u001B[33m\"\u001B[39m\u001B[33mval\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    325\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\models\\yolo\\classify\\train.py:147\u001B[39m, in \u001B[36mClassificationTrainer.get_dataloader\u001B[39m\u001B[34m(self, dataset_path, batch_size, rank, mode)\u001B[39m\n\u001B[32m    144\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch_distributed_zero_first(rank):  \u001B[38;5;66;03m# init dataset *.cache only once if DDP\u001B[39;00m\n\u001B[32m    145\u001B[39m     dataset = \u001B[38;5;28mself\u001B[39m.build_dataset(dataset_path, mode)\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m loader = \u001B[43mbuild_dataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdrop_last\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    148\u001B[39m \u001B[38;5;66;03m# Attach inference transforms\u001B[39;00m\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mode != \u001B[33m\"\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\data\\build.py:210\u001B[39m, in \u001B[36mbuild_dataloader\u001B[39m\u001B[34m(dataset, batch, workers, shuffle, rank, drop_last)\u001B[39m\n\u001B[32m    208\u001B[39m generator = torch.Generator()\n\u001B[32m    209\u001B[39m generator.manual_seed(\u001B[32m6148914691236517205\u001B[39m + RANK)\n\u001B[32m--> \u001B[39m\u001B[32m210\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mInfiniteDataLoader\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    211\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    212\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    213\u001B[39m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[43m=\u001B[49m\u001B[43mshuffle\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msampler\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    214\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    215\u001B[39m \u001B[43m    \u001B[49m\u001B[43msampler\u001B[49m\u001B[43m=\u001B[49m\u001B[43msampler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    216\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprefetch_factor\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnw\u001B[49m\u001B[43m \u001B[49m\u001B[43m>\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# increase over default 2\u001B[39;49;00m\n\u001B[32m    217\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpin_memory\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnd\u001B[49m\u001B[43m \u001B[49m\u001B[43m>\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcollate_fn\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcollate_fn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m    \u001B[49m\u001B[43mworker_init_fn\u001B[49m\u001B[43m=\u001B[49m\u001B[43mseed_worker\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    220\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgenerator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgenerator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    221\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdrop_last\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdrop_last\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m%\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[43m!=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    222\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\data\\build.py:66\u001B[39m, in \u001B[36mInfiniteDataLoader.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     64\u001B[39m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(*args, **kwargs)\n\u001B[32m     65\u001B[39m \u001B[38;5;28mobject\u001B[39m.\u001B[34m__setattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mbatch_sampler\u001B[39m\u001B[33m\"\u001B[39m, _RepeatSampler(\u001B[38;5;28mself\u001B[39m.batch_sampler))\n\u001B[32m---> \u001B[39m\u001B[32m66\u001B[39m \u001B[38;5;28mself\u001B[39m.iterator = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__iter__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:494\u001B[39m, in \u001B[36mDataLoader.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    492\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._iterator\n\u001B[32m    493\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m494\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:427\u001B[39m, in \u001B[36mDataLoader._get_iterator\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    425\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    426\u001B[39m     \u001B[38;5;28mself\u001B[39m.check_worker_number_rationality()\n\u001B[32m--> \u001B[39m\u001B[32m427\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_MultiProcessingDataLoaderIter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1172\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter.__init__\u001B[39m\u001B[34m(self, loader)\u001B[39m\n\u001B[32m   1165\u001B[39m w.daemon = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   1166\u001B[39m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[32m   1167\u001B[39m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[32m   1168\u001B[39m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[32m   1169\u001B[39m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[32m   1170\u001B[39m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[32m   1171\u001B[39m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1172\u001B[39m \u001B[43mw\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1173\u001B[39m \u001B[38;5;28mself\u001B[39m._index_queues.append(index_queue)\n\u001B[32m   1174\u001B[39m \u001B[38;5;28mself\u001B[39m._workers.append(w)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\process.py:121\u001B[39m, in \u001B[36mBaseProcess.start\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process._config.get(\u001B[33m'\u001B[39m\u001B[33mdaemon\u001B[39m\u001B[33m'\u001B[39m), \\\n\u001B[32m    119\u001B[39m        \u001B[33m'\u001B[39m\u001B[33mdaemonic processes are not allowed to have children\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    120\u001B[39m _cleanup()\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m \u001B[38;5;28mself\u001B[39m._popen = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    122\u001B[39m \u001B[38;5;28mself\u001B[39m._sentinel = \u001B[38;5;28mself\u001B[39m._popen.sentinel\n\u001B[32m    123\u001B[39m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[32m    124\u001B[39m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:224\u001B[39m, in \u001B[36mProcess._Popen\u001B[39m\u001B[34m(process_obj)\u001B[39m\n\u001B[32m    222\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m    223\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_Popen\u001B[39m(process_obj):\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_context\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mProcess\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:337\u001B[39m, in \u001B[36mSpawnProcess._Popen\u001B[39m\u001B[34m(process_obj)\u001B[39m\n\u001B[32m    334\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m    335\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_Popen\u001B[39m(process_obj):\n\u001B[32m    336\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpopen_spawn_win32\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[32m--> \u001B[39m\u001B[32m337\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001B[39m, in \u001B[36mPopen.__init__\u001B[39m\u001B[34m(self, process_obj)\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     94\u001B[39m     reduction.dump(prep_data, to_child)\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m     \u001B[43mreduction\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mto_child\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     97\u001B[39m     set_spawning_popen(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\reduction.py:60\u001B[39m, in \u001B[36mdump\u001B[39m\u001B[34m(obj, file, protocol)\u001B[39m\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdump\u001B[39m(obj, file, protocol=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m     59\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m60\u001B[39m     \u001B[43mForkingPickler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:26:39.971842Z",
     "start_time": "2025-10-16T02:26:39.089586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# ðŸš€ REPLACE your Temperature Scaling cell with this corrected version.\n",
    "#\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "print(\"--- Starting Temperature Scaling Calibration (v2) ---\")\n",
    "\n",
    "# --- âš™ï¸ Configuration ---\n",
    "CLASSIFIER_WEIGHTS_PATH = os.path.join(PROJECT_NAME, 'classifier_model/weights/best.pt')\n",
    "VALIDATION_DIR = os.path.join(CLASSIFIER_DATA_PATH, 'val')\n",
    "# ------------------------\n",
    "\n",
    "# 1. Load your trained classifier and validation data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "calib_model = YOLO(CLASSIFIER_WEIGHTS_PATH)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "val_dataset = ImageFolder(VALIDATION_DIR, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 2. Get the raw model outputs (logits) for the validation set\n",
    "logits_list = []\n",
    "labels_list = []\n",
    "print(\"Getting model outputs from validation set...\")\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        # Run prediction on the entire batch of images\n",
    "        results = calib_model.predict(images.to(device), verbose=False)\n",
    "\n",
    "        # --- THIS IS THE FIX ---\n",
    "        # Stack the probabilities from each result in the batch into a single tensor\n",
    "        # This creates a tensor of shape [batch_size, num_classes]\n",
    "        batch_probs = torch.stack([r.probs.data for r in results])\n",
    "\n",
    "        # Convert the batch's probabilities to logits\n",
    "        batch_logits = torch.log(batch_probs + 1e-9) # Add epsilon to avoid log(0)\n",
    "\n",
    "        # Append the entire batch's logits and labels to our lists\n",
    "        logits_list.append(batch_logits)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "# Concatenate all the batches into single tensors\n",
    "logits = torch.cat(logits_list).to(device)\n",
    "labels = torch.cat(labels_list).to(device)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}, Labels shape: {labels.shape}\") # Should be [N, 3] and [N]\n",
    "\n",
    "# 3. Find the optimal temperature 'T'\n",
    "def objective(T):\n",
    "    T = T[0]\n",
    "    # Ensure T is not zero to avoid division errors\n",
    "    if T == 0: return float('inf')\n",
    "    scaled_logits = logits / T\n",
    "    loss = F.cross_entropy(scaled_logits, labels.to(device))\n",
    "    return loss.item()\n",
    "\n",
    "print(\"Finding optimal temperature...\")\n",
    "result = minimize(objective, [1.0], method='nelder-mead', bounds=[(0.1, 10.0)])\n",
    "optimal_temperature = result.x[0]\n",
    "\n",
    "print(f\"\\nâœ… Calibration complete!\")\n",
    "print(f\"Optimal Temperature (T) = {optimal_temperature:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Temperature Scaling Calibration (v2) ---\n",
      "Getting model outputs from validation set...\n",
      "Logits shape: torch.Size([266, 3]), Labels shape: torch.Size([266])\n",
      "Finding optimal temperature...\n",
      "\n",
      "âœ… Calibration complete!\n",
      "Optimal Temperature (T) = 1.1410\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Pipeline Demonstration & Improvement Loop ðŸ\n",
    "--- \n",
    "This section shows how to use both models together and explains the next steps for improving your models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T02:42:59.249032Z",
     "start_time": "2025-10-16T02:42:53.658848Z"
    }
   },
   "source": [
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Running Full Pipeline with Simplified Stability Tracking ---\")\n",
    "\n",
    "# --- âš™ï¸ Configuration ---\n",
    "HISTORY_SIZE = 10\n",
    "IOU_MATCH_THRESHOLD = 0.4\n",
    "INACTIVE_FRAMES_LIMIT = 5\n",
    "MIN_CONFIDENCE = 0.50\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# --- Original Configuration ---\n",
    "DETECTOR_WEIGHTS_PATH = os.path.join(PROJECT_NAME, 'detector_model/weights/best.pt')\n",
    "CLASSIFIER_WEIGHTS_PATH = os.path.join(PROJECT_NAME, 'classifier_model/weights/best.pt')\n",
    "INPUT_VIDEO_PATH = 'C:\\\\Users\\\\josia\\\\Videos\\\\Marnie4.mp4' # <--- CHANGE THIS\n",
    "output_dir = 'pipeline_results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "OUTPUT_VIDEO_PATH = os.path.join(output_dir, f\"simple_stable_result_{os.path.basename(INPUT_VIDEO_PATH)}\")\n",
    "CAT_COLORS = {\"Marnie\": (0, 0, 255), \"Milo\": (255, 0, 0), \"Unknown\": (0, 255, 0)}\n",
    "DEFAULT_COLOR = (0, 255, 0) # Green fallback\n",
    "\n",
    "# --- Tracker State Variables ---\n",
    "tracked_objects = {}\n",
    "next_object_id = 0\n",
    "\n",
    "def calculate_iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "# 1. Load models\n",
    "print(\"Loading models...\")\n",
    "final_detector = YOLO(DETECTOR_WEIGHTS_PATH)\n",
    "final_classifier = YOLO(CLASSIFIER_WEIGHTS_PATH)\n",
    "print(\"âœ… Models loaded.\")\n",
    "\n",
    "# 2. Setup video reader and writer\n",
    "cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    print(f\"âŒ Error: Could not open video file at {INPUT_VIDEO_PATH}\")\n",
    "else:\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    print(f\"Processing video: {INPUT_VIDEO_PATH}\")\n",
    "\n",
    "    # 3. Loop through each frame\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        cat_detections = final_detector(frame, verbose=False)\n",
    "        current_detections = []\n",
    "        for detection in cat_detections:\n",
    "            for box_data in detection.boxes:\n",
    "                box = box_data.xyxy[0].int().tolist()\n",
    "                current_detections.append({'box': box, 'matched': False})\n",
    "\n",
    "        # --- Original (Greedy) Tracker Logic ---\n",
    "        for obj_id, obj_data in tracked_objects.items():\n",
    "            best_match_iou = 0\n",
    "            best_match_idx = -1\n",
    "            for i, det in enumerate(current_detections):\n",
    "                if not det['matched']:\n",
    "                    iou = calculate_iou(obj_data['box'], det['box'])\n",
    "                    if iou > best_match_iou:\n",
    "                        best_match_iou = iou\n",
    "                        best_match_idx = i\n",
    "\n",
    "            if best_match_iou > IOU_MATCH_THRESHOLD:\n",
    "                obj_data['box'] = current_detections[best_match_idx]['box']\n",
    "                obj_data['inactive_frames'] = 0\n",
    "                current_detections[best_match_idx]['matched'] = True\n",
    "                obj_data['updated_this_frame'] = True\n",
    "            else:\n",
    "                obj_data['inactive_frames'] += 1\n",
    "                obj_data['updated_this_frame'] = False\n",
    "\n",
    "        for det in current_detections:\n",
    "            if not det['matched']:\n",
    "                tracked_objects[next_object_id] = {\n",
    "                    'box': det['box'],\n",
    "                    'history': deque(maxlen=HISTORY_SIZE),\n",
    "                    'inactive_frames': 0,\n",
    "                    'updated_this_frame': True\n",
    "                }\n",
    "                next_object_id += 1\n",
    "\n",
    "        # --- Classification and Simplified Drawing ---\n",
    "        for obj_id, obj_data in tracked_objects.items():\n",
    "            if obj_data.get('updated_this_frame', False):\n",
    "                x1, y1, x2, y2 = obj_data['box']\n",
    "                cat_crop = frame[y1:y2, x1:x2]\n",
    "                if cat_crop.size == 0: continue\n",
    "\n",
    "                classification_results = final_classifier(cat_crop, verbose=False)\n",
    "                top_pred_index = classification_results[0].probs.top1\n",
    "                cat_name = final_classifier.names[top_pred_index]\n",
    "                confidence = classification_results[0].probs.top1conf.item()\n",
    "\n",
    "                obj_data['history'].append(cat_name)\n",
    "\n",
    "                most_common_name = max(set(obj_data['history']), key=list(obj_data['history']).count)\n",
    "                stability = list(obj_data['history']).count(most_common_name) / len(obj_data['history'])\n",
    "\n",
    "                adjusted_confidence = confidence * stability\n",
    "\n",
    "                if adjusted_confidence >= MIN_CONFIDENCE:\n",
    "                    label = f\"{most_common_name}: {adjusted_confidence:.2f}\"\n",
    "                    box_color = CAT_COLORS.get(most_common_name, DEFAULT_COLOR)\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), box_color, 2)\n",
    "                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, box_color, 2)\n",
    "\n",
    "        # --- Cleanup ---\n",
    "        inactive_ids = [obj_id for obj_id, data in tracked_objects.items() if data['inactive_frames'] >= INACTIVE_FRAMES_LIMIT]\n",
    "        for obj_id in inactive_ids:\n",
    "            del tracked_objects[obj_id]\n",
    "\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    video_writer.release()\n",
    "    print(f\"\\nâœ… Video processing complete! Output saved to: {OUTPUT_VIDEO_PATH}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Full Pipeline with Simplified Stability Tracking ---\n",
      "Loading models...\n",
      "âœ… Models loaded.\n",
      "Processing video: C:\\Users\\josia\\Videos\\Marnie4.mp4\n",
      "\n",
      "âœ… Video processing complete! Output saved to: pipeline_results\\simple_stable_result_Marnie4.mp4\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps & Improvement Loop\n",
    "\n",
    "1.  **Evaluate:** Use the demonstration cell above to visually inspect how well your pipeline works on different images.\n",
    "2.  **Gather More Data:** Use a script (like the one in your original notebook) to extract frames from new videos.\n",
    "3.  **Pre-label with the Detector:** Use your newly trained **detector** model (`detector_model/weights/best.pt`) to predict on these new frames. This will generate bounding boxes for any cats it finds.\n",
    "4.  **Import to Label Studio:** Import the new frames and their predicted labels into a new Label Studio project.\n",
    "5.  **Correct & Label:** In Label Studio, correct any bad bounding boxes and, most importantly, assign the correct class (`Marnie` or `Milo`) to each box.\n",
    "6.  **Re-run This Notebook:** Export your newly labeled data and run this entire notebook again. Your models will get smarter with each cycle!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import uuid\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"--- Starting Event Generation ---\")\n",
    "\n",
    "# --- âš™ï¸ Configuration ---\n",
    "DETECTOR_WEIGHTS_PATH = 'Cat_Pipeline_v1/detector_model/weights/best.pt'\n",
    "CLASSIFIER_WEIGHTS_PATH = 'Cat_Pipeline_v1/classifier_model/weights/best.pt'\n",
    "INPUT_VIDEO_PATH = 'C:\\\\Users\\\\josia\\\\Videos\\\\Milo4.mp4'  # <--- CHANGE THIS\n",
    "OUTPUT_DIR = 'seed_data'\n",
    "OUTPUT_CROPPED_IMAGE_DIR = os.path.join(OUTPUT_DIR, 'images', 'pending_review')\n",
    "OUTPUT_FULL_FRAME_IMAGE_DIR = os.path.join(OUTPUT_DIR, 'images', 'full_frames')\n",
    "OUTPUT_CSV_PATH = os.path.join(OUTPUT_DIR, 'events.csv')\n",
    "# -------------------------\n",
    "\n",
    "os.makedirs(OUTPUT_CROPPED_IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_FULL_FRAME_IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Loading models...\")\n",
    "final_detector = YOLO(DETECTOR_WEIGHTS_PATH)\n",
    "final_classifier = YOLO(CLASSIFIER_WEIGHTS_PATH)\n",
    "print(\"âœ… Models loaded.\")\n",
    "\n",
    "cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n",
    "event_data = []\n",
    "frame_number = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_number += 1\n",
    "    if frame_number % 15 != 0:\n",
    "        continue\n",
    "\n",
    "    original_height, original_width, _ = frame.shape\n",
    "    cat_detections = final_detector(frame, verbose=False)\n",
    "\n",
    "    # There can be multiple detections in a single result object\n",
    "    all_boxes = cat_detections[0].boxes\n",
    "    if all_boxes is None or len(all_boxes) == 0:\n",
    "        continue\n",
    "\n",
    "    # --- NEW: Save the full frame only ONCE if there are any cats ---\n",
    "    full_frame_id = str(uuid.uuid4())\n",
    "    full_frame_filename = f\"{full_frame_id}_full.jpg\"\n",
    "    full_frame_save_path = os.path.join(OUTPUT_FULL_FRAME_IMAGE_DIR, full_frame_filename)\n",
    "    cv2.imwrite(full_frame_save_path, frame)\n",
    "    full_frame_image_path_for_csv = f\"images/full_frames/{full_frame_filename}\"\n",
    "\n",
    "    # --- NEW: Loop through each detected box in the frame ---\n",
    "    for box in all_boxes.xyxy:\n",
    "        x1, y1, x2, y2 = [int(coord) for coord in box]\n",
    "\n",
    "        cat_crop = frame[y1:y2, x1:x2]\n",
    "        if cat_crop.size == 0: continue\n",
    "\n",
    "        classification_results = final_classifier(cat_crop, verbose=False)\n",
    "        top_pred_index = classification_results[0].probs.top1\n",
    "        cat_name = final_classifier.names[top_pred_index]\n",
    "        confidence = classification_results[0].probs.top1conf.item()\n",
    "\n",
    "        event_id = str(uuid.uuid4())\n",
    "\n",
    "        cropped_filename = f\"{event_id}_crop.jpg\"\n",
    "        cropped_save_path = os.path.join(OUTPUT_CROPPED_IMAGE_DIR, cropped_filename)\n",
    "        cv2.imwrite(cropped_save_path, cat_crop)\n",
    "\n",
    "        event_data.append([\n",
    "            event_id,\n",
    "            f\"images/pending_review/{cropped_filename}\",\n",
    "            full_frame_image_path_for_csv,  # Reference the single saved full frame\n",
    "            cat_name,\n",
    "            confidence,\n",
    "            x1, y1, x2, y2,\n",
    "            original_width,\n",
    "            original_height\n",
    "        ])\n",
    "\n",
    "print(f\"Writing {len(event_data)} events to {OUTPUT_CSV_PATH}...\")\n",
    "with open(OUTPUT_CSV_PATH, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        'Id', 'CroppedImagePath', 'FullFrameImagePath', 'PredictedClassName', 'PredictedConfidence',\n",
    "        'OriginalBboxX1', 'OriginalBboxY1', 'OriginalBboxX2', 'OriginalBboxY2',\n",
    "        'OriginalImageWidth', 'OriginalImageHeight'\n",
    "    ])\n",
    "    writer.writerows(event_data)\n",
    "\n",
    "cap.release()\n",
    "print(\"\\nâœ… Event generation complete!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
